# ğŸ“˜ **ZImagePipeline éå®˜æ–¹å®Œæ•´æ–‡æ¡£ï¼ˆ2025ï¼‰**

* ZImagePipeline çš„æ‰€æœ‰å‚æ•°è¯´æ˜
* æ¨ç†è¾“å…¥/è¾“å‡ºè§„èŒƒ
* Z-Image ç‰¹æœ‰ç‰¹æ€§
* æ”¯æŒçš„ä¼˜åŒ– backend
* Apple Silicon/MPS ä½¿ç”¨è¯´æ˜
* CUDA è¯´æ˜
* æœ€ä½³å®è·µï¼ˆBest Practicesï¼‰
* å®Œæ•´ä»£ç æ¨¡æ¿

---

# 1. ç®€ä»‹ï¼ˆOverviewï¼‰

`ZImagePipeline` æ˜¯ç”±é˜¿é‡Œé€šä¹‰å®éªŒå®¤ï¼ˆTongyi-MAIï¼‰ä¸º Diffusers æä¾›çš„å•æµ DiTï¼ˆS3-DiTï¼‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ Pipelineã€‚

å®ƒæ”¯æŒï¼š

* Z-Image Turboï¼ˆ8 NFEï¼‰âš¡
* ç²¾ç¡®ä¸­è‹±æ–‡æ¸²æŸ“
* é•¿ prompt ç†è§£
* é«˜é€Ÿæ¨ç†ï¼ˆFlash Attentionï¼‰
* bfloat16 è¿ç®—
* æŒ‡ä»¤å¯¹é½ï¼ˆInstruction followingï¼‰

æ­¤ pipeline çš„ API ä¸ StableDiffusionPipeline åŸºæœ¬ä¸€è‡´ï¼Œä½†æœ‰ä»¥ä¸‹åŒºåˆ«ï¼š

| ç‰¹æ€§             | ZImagePipeline                  | SDXL     | æ³¨é‡Š              |
| -------------- | ------------------------------- | -------- | --------------- |
| Text encoder   | Qwen2-VL tokenizer + text tower | CLIP     | æ”¯æŒä¸­æ–‡æ›´å¥½          |
| Model backbone | S3-DiT (Single-Stream DiT)      | UNet     | æ›´å¿«ã€æ›´ç°ä»£          |
| Guidance scale | `0.0` å›ºå®š                        | é€šå¸¸ 6â€“9   | Turbo æ¨¡å‹ä¸ä½¿ç”¨ CFG |
| Steps          | å›ºå®š 8~9                          | é€šå¸¸ 20â€“50 | è¶…é«˜é€Ÿç”Ÿæˆ           |
| Attention      | æ”¯æŒ Flash2/Flash3                | ä»…éƒ¨åˆ†æ¨¡å‹æ”¯æŒ  | Turbo åšäº†ä¼˜åŒ–      |

---

# 2. `ZImagePipeline.from_pretrained()` æ–‡æ¡£

```python
ZImagePipeline.from_pretrained(
    pretrained_model_name_or_path,
    *,
    torch_dtype=None,
    revision=None,
    subfolder=None,
    low_cpu_mem_usage=True,
    use_safetensors=True,
    cache_dir=None,
    local_files_only=False,
    resume_download=False,
    force_download=False,
    proxies=None,
    token=None
)
```

ä¸‹é¢æ˜¯é€é¡¹è§£é‡Šï¼š

---

## **å‚æ•°è¯¦è§£**

### **pretrained_model_name_or_path**

å­—ç¬¦ä¸² | å¿…å¡«

å¯ä»¥æ˜¯ï¼š

* HuggingFace æ¨¡å‹ ID
  `"Tongyi-MAI/Z-Image-Turbo"`
* æœ¬åœ°æ¨¡å‹ç›®å½•
  `"/Users/sean/zimage_cache/Z-Image-Turbo"`

---

### **torch_dtype**

é€‰æ‹©æ¨¡å‹æƒé‡ç±»å‹

æ¨èï¼š

| è®¾å¤‡                       | æ¨è dtype         |
| ------------------------ | ---------------- |
| NVIDIA GPU               | `torch.bfloat16` |
| Apple Silicon (M1/M2/M3) | `torch.float16`  |
| CPU                      | `torch.float32`  |

ç¤ºä¾‹ï¼š

```python
torch_dtype=torch.bfloat16
```

---

### **low_cpu_mem_usage**

æ˜¯å¦åˆ†æ®µåŠ è½½æ¨¡å‹ä»¥å‡å°‘å†…å­˜å ç”¨ã€‚

Z-Image å‚æ•°è¾ƒå¤§ï¼ˆ6Bï¼‰ï¼Œå»ºè®®ï¼š

```python
low_cpu_mem_usage=True
```

---

### **cache_dir**

æŒ‡å®šæ¨¡å‹ä¸‹è½½ç›®å½•ã€‚

ç¤ºä¾‹ï¼š

```python
cache_dir="/Users/sean/zimage_models"
```

---

### **local_files_only**

ä»…ä»æœ¬åœ°åŠ è½½ï¼Œä¸è”ç½‘ã€‚

```python
local_files_only=True
```

---

### **force_download**

å¼ºåˆ¶é‡æ–°ä¸‹è½½æ¨¡å‹ã€‚

---

## **è¿”å›å€¼**

`ZImagePipeline` å®ä¾‹ï¼ŒåŒ…å«ï¼š

* `pipe.text_encoder`ï¼ˆQwen2-VL æ–‡æœ¬å¡”ï¼‰
* `pipe.transformer`ï¼ˆS3-DiT backboneï¼‰
* `pipe.vae`ï¼ˆAutoEncoder KLï¼‰
* `pipe.tokenizer`
* `pipe.scheduler`ï¼ˆDMD ä¸“ç”¨ schedulerï¼‰

---

# 3. `.to(device)` è¯´æ˜

Z-ImagePipeline æ”¯æŒï¼š

| device   | æ˜¯å¦æ”¯æŒ | å¤‡æ³¨            |
| -------- | ---- | ------------- |
| `"cuda"` | âœ”ï¸   | å…¨é€Ÿ            |
| `"mps"`  | âœ”ï¸   | Apple Silicon |
| `"cpu"`  | âœ”ï¸   | æ…¢             |

ä¾‹ï¼š

```python
pipe.to("mps")  # Apple Silicon
pipe.to("cuda") # NVIDIA
```

---

# 4. Attention Backend

ä½ å¯ä»¥åˆ‡æ¢ Flash Attentionï¼š

```python
pipe.transformer.set_attention_backend("flash")      # Flash-2
pipe.transformer.set_attention_backend("_flash_3")   # Flash-3
```

---

# 5. æ¨ç†æ¥å£ï¼ˆcallï¼‰

```python
pipe(
    prompt: str,
    *,
    height: int = 1024,
    width: int = 1024,
    num_inference_steps: int = 9,
    guidance_scale: float = 0.0,
    generator=None,
)
```

### å‚æ•°è¯´æ˜

| å‚æ•°                  | é»˜è®¤   | è¯´æ˜              |
| ------------------- | ---- | --------------- |
| prompt              | å¿…å¡«   | æ–‡æœ¬ promptï¼ˆæ”¯æŒä¸­è‹±ï¼‰ |
| height              | 1024 | ç”Ÿæˆå›¾åƒé«˜åº¦          |
| width               | 1024 | ç”Ÿæˆå›¾åƒå®½åº¦          |
| num_inference_steps | 9    | DMD few-step    |
| guidance_scale      | 0.0  | Turbo å›ºå®šä¸º `0.0` |
| generator           | None | æ§åˆ¶éšæœº seed       |

ä¾‹ï¼š

```python
image = pipe(
    prompt=prompt,
    height=1024,
    width=1024,
    num_inference_steps=9,
    guidance_scale=0.0,
    generator=torch.Generator("mps").manual_seed(42),
).images[0]
```

è¾“å‡ºï¼š

* `image` â†’ PIL Image

---

# 6. Z-Image Turbo æ¨èé…ç½®ï¼ˆBest Practiceï¼‰

## â‘  dtype

| GPU           | æ¨è dtype |
| ------------- | -------- |
| NVIDIA        | bfloat16 |
| Apple Silicon | float16  |
| CPU           | float32  |

---

## â‘¡ steps

Turbo å›ºå®šï¼š

```python
num_inference_steps=9  # å®é™…8 forward
```

---

## â‘¢ guidance

Turbo å¿…é¡»ï¼š

```python
guidance_scale=0.0
```

---

## â‘£ prompt engineering

çŸ­ promptï¼ˆ10â€“40 tokensï¼‰æ•ˆæœæœ€å¥½ã€‚
é•¿ prompt ä¹Ÿèƒ½å¤„ç†ï¼Œä½† Turbo æ˜¯é€Ÿæ¨æ¨¡å‹ã€‚

---

# 7. Apple Silicon ç‰¹åˆ«è¯´æ˜

M1/M2/M3 ä½¿ç”¨ï¼š

```python
pipe.to("mps")
```

æœ€ä½³ dtypeï¼š

```python
torch_dtype=torch.float16
```

ä¸æ”¯æŒ FlashAttentionï¼ˆMetal æ—  Flashï¼‰ã€‚

æ¨ç†é€Ÿåº¦çº¦ï¼š

* M2 Maxï¼šæ¯å¼  ~2.5â€“4 ç§’ï¼ˆ1024Ã—1024ï¼‰
* M3 Maxï¼šå¯è¾¾ ~1.6â€“2.0 ç§’

---

# 8. å®Œæ•´ç¤ºä¾‹ä»£ç ï¼ˆå¯ç›´æ¥è¿è¡Œï¼‰

```python
import torch
from diffusers import ZImagePipeline

model_id = "Tongyi-MAI/Z-Image-Turbo"

pipe = ZImagePipeline.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
)

pipe.to("cuda")  # Apple Silicon ç”¨ "mps"

prompt = "A young Chinese woman in red hanfu with phoenix hairpin, vivid neon lighting."

image = pipe(
    prompt=prompt,
    height=1024,
    width=1024,
    num_inference_steps=9,
    guidance_scale=0.0,
    generator=torch.Generator("cuda").manual_seed(42),
).images[0]

image.save("output.png")
```

---

# 9. Z-Image Turbo ç›®å½•ç»“æ„

`from_pretrained` åŠ è½½çš„æ¨¡å‹ç›®å½•åº”åŒ…å«ï¼š

```
config.json
model_index.json
tokenizer/
text_encoder/
vae/
transformer/
scheduler/
```

è¿™ä¸ ComfyUI çš„ `.safetensors` checkpoint **å®Œå…¨ä¸åŒ**ã€‚
